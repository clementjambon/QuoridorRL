\documentclass[journal, a4paper]{IEEEtran}

\usepackage{graphicx}   
\usepackage{hyperref} 
\usepackage{url}        
\usepackage{amssymb}
\usepackage{amsmath}    

% Some useful/example abbreviations for writing math
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\ypred}{\mathbf{\hat y}}
\newcommand{\yp}{{\hat y}}

\newif\ifanonymous
\anonymoustrue

\begin{document}

% Define document title, do NOT write author names for the initial submission
\title{QuoridorRL: solving a two-player strategy game with reinforcement learning}
\ifanonymous
\author{Anonymous Authors}
\else
\author{Nathan Pollet, Rebecca Jaubert, Laura Minkova, Erwan Umlil and ClÃ©ment Jambon}
\fi
\maketitle

% Write abstract here
\begin{abstract}

\end{abstract}

% Each section begins with a \section{title} command
\section{Introduction}
\label{sec:intro}

- What aspect? = Zero-sum two-player game = Quoridor
- Why important? = Normally, use MinMax + heuristics but here, very large complexity, not much work before, 
- What challenges? = no human knowledge/pre-existing game data (contrary to Go, Shogi or Chess which have already been widely studied) => we cannot use supervised learning approaches (such as AlphaGo\cite{alphago}) / large state space, large branching spaces which bound exhaustive tree search methods to failure (e.g MinMax/AlphaBetaPruning)
- => heuristics, MC-RAVE (Rapid Action Value Estimation), AlphaZero (self-play+evaluation network to guide the expansion of the MCTS)
- Our model still training due to the large complexity, and trouble tuning the hyperparameters, especially for alphazero


In addition to this paper, we provide the sources of our code at  \footnote{Source: \url{}}. 

In the following, we first review prior approaches to provide robust and human-level control agents in two-player games with large complexities (section \ref{sec:background}), then we describe our implementation of the environment (section \ref{sec:environment}) and the models we used to solve it (section \ref{sec:models}), followed lastly by our experiments and resulting discussion (section \ref{sec:results}).

\section{Background and Related Work}
\label{sec:background}

\subsection{Game and complexity}
\label{ssec:complexity}
Describe the state and branching complexity of the game.
https://math.stackexchange.com/questions/953750/counting-all-possible-board-positions-in-quoridor
https://project.dke.maastrichtuniversity.nl/games/files/bsc/Mertens_BSc-paper.pdf

\subsection{MCTS}
\label{ssec:mcts}
Use paper Erwan \cite{mc-rave}

For a review of MCTS methods, refer to \cite{mcts-review}.

\subsection{Training without human knowledge}
\label{ssec:human-knowledge}

As seen in paragraph \ref{ssec:complexity}, \textit{Quoridor} is a game with very high state and branching complexities. Consequently, given the existing computational resources and considering that a decision must be provided by our agent in a moderately realistic amount of time, we cannot perform an exhaustive tree search such as MinMax. Therefore, we turned to an \textit{AlphaGo} approach as presented in \cite{alphago}.

In order to perform Monte Carlo Tree Search over the large complexity space of \textit{Quoridor} and considering the lack of game data contrary to well-known and already analysed, we faced the challenge of training our model without human knowledge. To this extent, we turned to the \textit{"self-play"} approach initially presented in  \textit{AlphaGoZero}\cite{alphagozero} and later expanded to a larger set of games (namely Chess and Shogi) in \textit{AlphaZero}\cite{alphazero}.

\section{Environment}
\label{sec:environment}

As an introduction = environment set up from scratch => coding the came logics + choose consistent state and action spaces representations (and/or) wrapper

In order to limit the computational burden due to the very large complexity of the game in its original configuration, we chose to simplify the state space by restricting ourselves to the following rules:
\begin{itemize}
    \item the board has size $5\times 5$ (instead of $9\times 9$)
    \item each player can add at most $5$ walls (instead of $10$)
    \item the game can last at most $100$ turns, after which it is considered a "draw"
\end{itemize}

\subsection{Game logic}
Although the game logic could be implemented by relying on a graph as suggested in (cf. Rebecca), we chose a different intrinsic representation.
Game logic = coded from scratch. Although an easy, 

\subsection{State and action representations}
The implemented logic of 
Intrinsic representation = 

\subsection{User interface}
In order to visualize and play games abstracted by the aforementioned logic, we provide a user interface with parameters that can be tuned according to the chosen configuration of the game. To this extent, we used the python \textit{SDL}-based library \textit{PyGame}\cite{pygame}.


\section{Models}
\label{sec:models}

\subsection{AlphaZero agent}

As mentioned in section \ref{sec:background}, \textit{Quoridor} is a typical example of a two-player zero-sum game with a very high complexity. As a consequence
+ choice of architecture (see appendix)

In evaluation mode, a time parameter can be given and limit the MCTS (i.e. tree searches will be performed as long as the agent can do it)


\section{Results and Discussion}
\label{sec:results}


\section{Conclusions}
\label{sec:conclusion}

\bibliographystyle{plain}
\bibliography{biblio}

\newpage
\section*{Appendix}
This is the place to put work that you did but is not essential to understand the paper: additional results and tables, lengthy proofs and derivations, \ldots. Material here does not count towards page limit (but also it will be optional for the reviewer/teacher to work through). 
\end{document}
