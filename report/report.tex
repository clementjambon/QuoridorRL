\documentclass[journal, a4paper]{IEEEtran}

\usepackage{graphicx}   
\usepackage{hyperref} 
\usepackage{url}        
\usepackage{amssymb}
\usepackage{amsmath}    
\usepackage{booktabs} % Essential for nice tabs (see https://people.inf.ethz.ch/markusp/teaching/guides/guide-tables.pdf)
\usepackage{xcolor} 

\usepackage{bm}
\newcommand{\vect}[1]{\boldsymbol{\mathbf{#1}}}

% Some useful/example abbreviations for writing math
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\ypred}{\mathbf{\hat y}}
\newcommand{\yp}{{\hat y}}

\newif\ifanonymous
\anonymoustrue

\begin{document}

% Define document title, do NOT write author names for the initial submission
\title{QuoridorRL: solving a two-player strategy game with reinforcement learning}
\ifanonymous
\author{Anonymous Authors}
\else
\author{Nathan Pollet, Rebecca Jaubert, Laura Minkova, Erwan Umlil and Cl√©ment Jambon}
\fi
\maketitle

% Write abstract here
\begin{abstract}
Quoridor is a zero-sum two-player board game which has not been extensively studied yet in the context of reinforcement learning. Its relatively large state and branching complexities and the lack of human-generated data make it very challenging and unsuitable for simple control strategies and exhaustive tree search approaches. Consequently, we provide a full \textit{gym}-like environment for the game and its mechanics. We then introduce three strategies with the aim of reaching human-level control: first a heuristic approach with \textit{MinMax} tree search relying on handcrafted evaluation functions, then a \textit{Monte-Carlo Tree Search} with \textit{Rapid Action Value Estimation} and finally a \textit{Monte-Carlo Tree Search} augmented with an evaluation neural network directly inspired from \textit{AlphaZero}. Even though, we have not reached convincing results yet, we are confident such approaches can yield satisfying performances with more computational resources.
\end{abstract}

% Each section begins with a \section{title} command
\section{Introduction}
\label{sec:intro}

In this project, game theory, and in particular reinforcement learning approaches, are considered through the concrete example of Quoridor, a complex two-player zero-sum strategy game designed by Mirko Marchesi and published by Gigamic Games \cite{quoridor-gigamic}. This is a particularly interesting use case as its game complexity is very large (large state and branching complexities), which makes tree search approaches (e.g. minimax, heuristics-based approaches) impractical for real-time applications. Moreover, finding or generating sufficient amounts of game-related expert data for use in supervised learning methods is a challenging task. Large game complexity is a common challenge in popular games such as chess, Go or Shogi whereas the scarcity of expert game data is rather rare for most of these popular games, the majority of which having been played and studied widely. For instance \textit{AlphaGo} \cite{alphago}, whose successor \textit{AlphaZero} \cite{alphazero} is the basis of one of the three strategies implemented, could not be repurposed for this game scenario due to a lack of proper data. To implement our agents, more efficient search approaches are used such as \textit{Monte-Carlo Tree Search} with \textit{Rapid Action Value Estimation} (\textit{MC-RAVE}) \cite{GELLY20111856} and \textit{AlphaZero} \cite{alphazero}. In this paper, prior approaches to provide robust and human-level control agents are reviewed (section \ref{sec:background}), the implementation of the environment is discussed (section \ref{sec:environment}), methods used to solve the environment are detailed (section \ref{sec:models}), the experiments are described and a discussion of the results is presented (section \ref{sec:results}).
 Although the environment and the agents are successfully implemented, due to the large complexity of the game some agent implementations such as \textit{AlphaZero} require further training and refinements (e.g. minor structural changes, hyperparameter tuning). 
%Lastly, time permitting, additional agent implementations the likes of SARSA, Q-Learning or DQN will be considered in the context of an overall comparative analysis. % should we keep this here? vs in Conclusions?

In addition to this paper, we provide the sources of our code\footnote{Source: \url{}} along with instructions to run each pipeline in the attached \textit{README} file. 

\section{Background and Related Work}
\label{sec:background}

\subsection{Game and complexity}
\label{ssec:complexity}

Quoridor is a stochastic, perfect information, adversarial game that can be played with two to four players. For the purpose of this project and exploring reinforcement learning methods, however, only the two-player version of the game will be considered. The board game consists of a $n$ by $n$ square-tiled board, where $n$ is odd. 
The two players start off with $n+1$ rectangular tiles and one game pawn each. Each player then places their respective pawn in the center of opposing outermost sides of the grid. Both players take turns doing one of two things: moving their pawn by one spot or placing a wall on the board in the spaces between the tiles, making sure the length of the wall is in contact with exactly two tiles on each side. 
The objective of the game is to advance your respective pawn from one end of the board to the other. Players can place the walls anywhere on the grid, but at least one viable path to each side must exist for each player. Details on legal pawn moves can be found in \ref{sec:quoridor-details}.

As previously mentioned, Quoridor has very large state-space and game tree complexities. The following paragraph will be dedicated to estimating these values. 

The state-space of a game is defined as all the possible states of the game \cite{heuristic-agent}. Given the difficulty in calculating the number of illegal positions in a given state of Quoridor, an upper bound for all possible states will be calculated. 
This upper-bound is equal to the product of the total number of possible pawn positions, $S_{tok}$, and the total number of wall positions, $S_{wall}$.  There are at most $n^{2}$ positions that the first player's pawn can be placed at, and $n^{2} -1$ possible positions on the board for the second player to place their game piece. Therefore $S_{tok} = n^{2}(n^{2}-1) = n^{4} - n^{2}$.

The number of possible wall positions can also be estimated using an upper-bound. In a single row, $n-1$ walls can be placed. Since there are $n-1$ rows, there are $(n-1)^{2}$ possible places to put walls on the rows of the grid. One must consider the possible positions of walls in the columns, which as it turns out is the exact same given the square nature of the grid. Therefore there are technically $2(n-1)^{2}$ possible places to put walls in Quoridor. 
Placing a wall at any position on the grid, however, takes away at most four possible wall positions from the total possible spots previously calculated. Thus, for the $2(n+1)$ fences that can be placed on the board (since each player has $n+1$ rectangular tiles, there are at most $\sum_{i=0}^{2n} \prod_{j=0}^{i}(2(n-1)^2 - 4i)$ possible placements for the walls. This is clearly a loose upper-bound as it misses many other illegal positions of walls, but it does give an idea of the scale of the number of possible walls. Finally, the state space is equal to $2(n-1)^{2} * \sum_{i=0}^{2n} \prod_{j=0}^{i}(2(n-1)^2 - 4i)$, which can grow incredibly large as the size of the board increases.

The size of a game tree is the number of possible games that can be played. This can be estimated by the value $b^{d}$ where $b$ is the average number of branches there are from a node in the game tree (i.e possible positions from a given state), and $d$ is the average number of turns that a game takes (i.e the average depth of the game tree). One can see that as the board gets larger, the number of possible moves from a state space grows due to the increasing number of possible positions for both token and walls, as does the number of turns a game may take since the token will have to make increasingly more steps to get to the other side. 
Therefore, the size of the game tree grows exponentially. For a 9 by 9 board, for example, the size of the game-tree can be estimated as $60.4^{91.1} \approx 1.7884 * 10^{162}$ \cite{mastering-quoridor}, which makes exhaustive search impossible.

% https://math.stackexchange.com/questions/953750/counting-all-possible-board-positions-in-quoridor

\subsection{MCTS}
\label{ssec:mcts}

Monte Carlo Tree Search (MCTS)\cite{mcts-review} is a search method that combines the precision of tree search with the generality of random sampling. By minimizing the search space it is an efficient approach which is, due to its property of growing the tree asymmetrically by concentrating on the more promising subtrees, particularly well suited for games with a high branching factor. Moreover, it can function with little or no domain knowledge and is a particularly proficient approach for games and problems which can be represented as trees of sequential decisions. MCTS operates by taking random samples in the decision space and building a search tree according to the results. The basic MCTS process is straightforward. The tree is built incrementally and asymmetrically. For each iteration of the algorithm, MCTS leverages a tree policy in order to select the "best" node for the current tree for expansion. A good policy has a proper balance between exploration (explore yet unsampled areas) and exploitation (explore promising areas). Then, a simulation is ran from the selected node and the (current) search tree is updated from its results (addition of a child node, update of the statistics of its ancestors). 

Various modifications of the basic Monte Carlo tree search method have been proposed, mostly to shorten the search time. Certain improvements attempt to, when applicable, use a heavy payout to incorporate various heuristics that influence the choices of selected nodes (i.e. the choices of moves). These heuristics could employ domain knowledge or use the results of previous playouts (e.g. the \textit{Last Good Reply heuristic}, \textit{history heuristics}). Another approach is to employ domain knowledge when building the game tree to help the exploitation of some variants. Such an approach is related to the concept of \textit{progressive bias}. Finally, using the \textit{Rapid Action Value Estimation} (\textit{RAVE}) can reduce the exploratory phase of the algorithm significantly by modifying the statistics which is being stored at the nodes of interest (i.e. modified \textit{UCB1} formula). This approach is being implemented for one of the agents. 

\subsection{Training without human knowledge}
\label{ssec:human-knowledge}

As seen in paragraph \ref{ssec:complexity}, \textit{Quoridor} is a game with very high state and branching complexities. Consequently, given the existing computational resources and considering that a decision must be provided by our agent in a moderately realistic amount of time, we cannot perform an exhaustive tree search such as MinMax. Therefore, we turned to an \textit{AlphaGo} approach as presented in \cite{alphago}.

In order to perform Monte-Carlo Tree Search over the large complexity space of \textit{Quoridor} and considering the lack of game data contrary to well-known and already analysed, we faced the challenge of training our model without human knowledge. To this extent, we chose a \textit{"self-play"} approach similar to the one initially presented in  \textit{AlphaGoZero}\cite{alphagozero} and later expanded to a larger set of games (namely Chess and Shogi) in \textit{AlphaZero}\cite{alphazero}. Furthermore, unlike \textit{AlphaGo}, \textit{AlphaZero} does not require handcrafted features and is thus more suited to the problem of solving a new game from scratch, as its neural network learns itself to capture features through residual convolutions of the game board. 

\section{Environment}
\label{sec:environment}

 \textit{Quoridor} being a little-known board game, there is, to our knowledge, no public implementation of its mechanics for the purpose of training RL agents. As a consequence, we provide with this project a turn-based environment with a \textit{gym}-like (cf. \cite{openai-gym})

In order to limit the computational burden due to the very large complexity of the game in its original configuration, we chose to simplify the state space by restricting ourselves to the following rules:
\begin{itemize}
    \item the board has size $5\times 5$ (instead of $9\times 9$)
    \item each player can add at most $5$ walls (instead of $10$)
    \item the game can last at most $100$ turns, after which it is considered a "draw"
\end{itemize}

\subsection{Logic and representation}
\label{ssec:game-logic}

Although the game logic could be implemented by relying on a graph as suggested in \cite{heuristic-agent}, we chose a different intrinsic representation to limit the memory and computational footprint which are critical for efficient \textit{Monte-Carlo Tree Searches}. We coded the game logic from scratch and provide an interface that allows users to check whether an action is valid or not, fetch all valid actions, take actions based on abstract representation or the string representation mentioned below. In order to check valid actions, we use the well-known $A^*$ algorithm (see chapter 3 of \cite{russel2010} for more details) in order to perform pathfinding. Note that every time we test an action, a pathfinding is performed even though little changes on the board have occured from one state to the other ; we should thus consider in future works a more robust solution leveraging previous computations.

Intrinsiquely, the state $s\in\mathcal{S}$ of the game is represented as the set of player positions on the board, the number of remaining walls for each player, the index of the current player and a 2D array storing walls located by their lower left-hand positions and where $-1, 0, 1$ stands respectively for empty, horizontal ($x$-aligned), vertical ($y$-aligned).

States and actions can also be represented in a \textit{string} notation. Ours is inspired from the algebraic notation presented in \cite{quoridor-wikipedia}. More precisely, the $x$-coordinate of states are indexed in $\{1, \ldots, grid\_ size\}$ while the $y$-coordinate of states are indexed in $\{a, b, c, \ldots\}$. Walls are described in a similar fashion by appending the direction ($h$ for horizontal and $v$ for vertical) to the number-letter position couple. As such, we can represent player actions by the target \textit{string} position when the player performs a pawn move or the target wall position with the direction suffix when the player adds a wall. For completeness, we add the number of used walls to the state and the current player at the end of the \textit{string} representation. Equation \ref{eq:state-str} provides an example of such a representation and is illustrated in figure \ref{fig:state-str}.
\begin{equation}
    \label{eq:state-str}
    % 1d;5c;1dv;2bh;3av;3bv;4dv;p0:2;p1:3;p0;
    \underbrace{\text{1d;5c;}}_\text{pawn positions}\overbrace{\text{1dv;2bh;3av;3bv;4dv;}}^\text{walls}\underbrace{\text{p0:2;p1:3;}}_\text{used walls}\overbrace{\text{p0;}}^\text{current player}
\end{equation}
\begin{figure}
    \centering
    \includegraphics[width=0.3\textwidth]{figures/state-str.png}
    \caption{Board representation of the state given in equation \ref{eq:state-str}}
    \label{fig:state-str}
\end{figure}

\subsection{User interface}
In order to visualize and play games abstracted by the aforementioned logic, we provide a user interface with parameters that can be tuned according to the chosen configuration of the game. To this extent, we used the python \textit{SDL}-based library \textit{PyGame}\cite{pygame}. Our relatively simple yet functional \textit{GUI} is shown in figure \ref{fig:gui}.
\begin{figure}
    \centering
    \includegraphics[width=0.3\textwidth]{figures/gui.png}
    \caption{Provided user interface}
    \label{fig:gui}
\end{figure}


\section{Models}
\label{sec:models}

\subsection{Decision Making, MinMax and Heuristics}\label{ssec:heuristics}

This section discusses the basic decision making methods developed for our agent. There are two strategies the agent can take whose usages depend on two factors: the opponent has no more walls to place on the board, or they still do.
In the former, if no more walls can be placed on the board by your opponent the agent proceeds by moving in the shortest-path manner which is calculated with the Djistra method. 
In the latter, we base our next move using a MinMax tree.

The MinMax search tree is a classical method for decision making in a zero-sum game setting. Every edge in the tree is an action or move in the game and every node stores the evaluation of a particular state of the game.
Ideally, a full MinMax tree can be computed from which, starting from the leaf nodes, you chose the action that maximizes or minimizes the reward depending on if it is your turn or not. As previously mentionned in \ref{ssec:complexity}, Quoridor has a very large game tree. For this reason, we chose to compute the MinMax tree only up to a certain depth, and evaluate a game state given four different heuristics: 1. $f_1$ the position of the current player, 2. $f_2$ the position difference of the players, 3. $f_3$ the number of required moves for the current player to get to the next column and 4. $f_4$ the number of required moves for the opposing player to get to the next column. These four features are then combined into a single game state value using the following formula:  
$$Eval(s) = w_1 f_1 + w_2 f_2 + w_3 f_3 - w_4 f_4,   w_i \in \mathbb{R}$$
This will result in two things: we will be able to successfully and efficiently perform a MinMax analysis, and we will be able to use the calculated state heuristics for not only MinMax but MCTS-RAVE as well.

\subsection{MC-RAVE}
\label{ssec:mc-rave}
In order to solve complex two-player strategy games, \textit{MCTS} has proven to be useful (cf. \cite{mcts-review}). However, when the state and branching complexities grow large, more samples need to be generated. \textit{MC-RAVE}\cite{mc-rave} tackles this issue by extrapolating on data collected w.r.t. the action space.

More particularly, in \textit{MCTS}, we simulate by self-play $N(s)$ games from state $s$. For a given state $s$ and an action $a$, the value fonction can then be approximated with:
$$Q(s,a) = \frac{1}{N(s,a)} \sum_{i=1}^{N(s)} \mathbf{1}_{i}(s,a)z_i$$
where $N(s)$ is the number of simulations starting from state $s$, $\mathbf{1}_{i}(s,a)$ an indicator function returning 1 if action $a$ was selected in state $s$ during the $i$-th simulation, $N(s,a):=\sum_{i=1}^{N(s)} \mathbf{1}_{i}(s,a)$ the number of simulations in which action $a$ was selected in state $s$, and $z_i$ the final outcome of the $i$-th simulation.

\textit{MCTS} consists in updating a \textit{search tree}. For each simulation, we use either a \textit{tree policy} or a \textit{default policy} to choose the next action. The tree policy is used when the current state is represented in the search tree and boils down to the selection of the best child. Otherwise, a default policy is used: we select at random one action among the set of possible actions.

The main issue with this algorithm lies in the huge number of simulations required to estimate the action value $Q(s,a)$. \textit{Rapid Action Value Estimation} (\textit{RAVE}) \cite{mc-rave} tackles this issue by providing a way to quickly estimate the value of an action thanks to the \textit{all-moves-as-first} (\textit{AMAF}) heuristic. The main idea is that to assume that each action has a global value, no matter when it is played. Therefore, we can estimate the value of an action $a$ in state $s$ as follows:
$$\tilde Q(s,a) = \frac{1}{\tilde N(s,a)} \sum_{i=1}^{N(s)} \mathbf{\tilde 1}_{i}(s,a)z_i$$
where $\mathbf{\tilde 1}_{i}(s,a)$ is an indicator function returning 1 if state $s$ was encountered at any step $t$ of the $i$-th simulation, and action $a$ was selected at any state $u\geq t$, and $\tilde N(s,a):=\sum_{i=1}^{N(s)} \mathbf{\tilde 1}_{i}(s,a)$ the number of simulations used to estimate the \textit{AMAF} value. So in \textit{RAVE} algorithm, rather than looking at simulations where action $a$ was selected in state $s$, we look at all simulations where action $a$ was selected \textit{after} state $s$. It is faster to compute, but less accurate.

\textit{MC-RAVE} simply consists in a trade-off between conventional \textit{MCTS} and \textit{RAVE}, by mixing both estimated action values:
$$Q_*(s,a) = (1-\beta(s,a))Q(s,a) + \beta(s,a)\tilde Q(s,a)$$
We use a handcrafted schedule proposed in the same paper\cite{mc-rave}:
$$\beta(s,a) = \sqrt[]{\frac{k}{3N(s)+k}}$$
with $k=1000$.

Finally, we use \textit{UCT-RAVE} \cite{mc-rave} to favor exploration:
$$Q_*^+(s,a) = Q_*(s,a) + c\:\:\sqrt[]{\frac{\log N(s)}{N(s,a)}}$$
with $c$  as a mixing parameter.

\subsection{AlphaZero agent}
\label{ssec:alphazero}
    To alleviate the computational burden of tree search, \textit{AlphaZero}\cite{alphazero} proposes to perform \textit{MCTS} where, instead of full rollouts like in \textit{MC-RAVE}, nodes are progressively expanded thanks to a single policy-value evaluation network $f_\theta(s)=(\mathbf{p}, v)$ that takes as input a state $s\in \mathcal{S}$ and provides an action policy $\mathbf{p}\in\mathcal{P}(\mathcal{A})$ (see appendix \ref{sec:alphazero-network} for the detailed architecture). Similarly to conventional \textit{MCTS}, each node represents a state and a edge stands for a state-action pair whose attributes are $N(s,a)$ its number of visit, $W(s,a)$ the accumulated action value, $Q(s,a)$ the average action value and $P(s,a)$ the prior probability of this pair. With this setup, we follow iteratively three consecutive steps:
    \begin{enumerate}
        \item \textbf{Search:} we start from an uninitialized tree whose single root node is the current state $s_0$ of the simulation. For every simulation, as long as a leaf as not been reached, the tree is explored by choosing actions according \textit{UCT} (Upper Confidence bounds applied to Trees) algorithm that treats the tree as a multi-armed bandit problem and maximizes an upper confidence bound on the value of actions : $$a_t=\argmax_{a\in\mathcal{A}}(Q(s_t, a)+c_{uct}P(s_t,a)\frac{\sqrt{\sum_{b\in\mathcal{A}}N(s_t, b)}}{1+N(s_t,a)})$$ where $c_{uct}$ enables tuning between greedy exploitation and exploration of the tree.
        \item \textbf{Expansion:} everytime a leaf $s_L$ is encountered in the rollout, it is expanded by adding edges following the estimation provided by the neural network $f_\theta(s_L)=(\mathbf{p_L}, v_l)$ i.e each edge is initialized with $N(s_L, a)=W(s_L,a)=Q(s_L,a)=0$ and $P(s_L,.)=\mathbf{p_L}$.
        \item \textbf{Backup:} the action values and visits of each rollout are back-propagated following $N(s_t, a_t) = N(s_t, a_t) + 1$, $W(s_t, a_t) = W(s_t, a_t) + v$ and $Q(s_t, a_t)=\frac{W(s_t, a_t)}{N(s_t, a_t)}$
    \end{enumerate}


    Unlike \textit{AlphaGo}\cite{alphago} whose expansion was guided by a network trained by supervision on real human-generated data, \textit{AlphaZero} uses self-play schemes to generate training samples without human knowledge which makes it particularly interesting in the context of \textit{Quoridor} for which we have no collected data. We adapted these schemes to our \textit{Quoridor} environment as follows. We first randomly initialize the evaluation network $f_{\theta_0}$ with random weights $\theta_0$ and then proceed iteratively:
    \begin{enumerate}
        \item the last trained network $f_{\theta_i}$ is used to generate self-play games where each move is chosen according to a \textit{MCTS} as described above. More precisely, actions are chosen following the search policy given by the visit counts of the root edges accumulated during the search and tempered with a temperature parameter $\tau$ i.e. $\boldsymbol{\pi}(.|s_0)=\frac{N(s_0,.)^\frac{1}{\tau}}{\sum_{b\in\mathcal{A}}N(s_0, b)^\frac{1}{\tau}}$ and we record state-action-outcome tuples $(s_t, \boldsymbol{\pi}_t, z)$ where $z$ is $1, -1, 0$ respectively for a victory, a defeat or a draw.
        \item the network is $f_{\theta_{i+1}}$ is initialized with the weights of $f_{\theta_i}$ and trained with batches of self-play records sampled by previous models i.e $f_{\theta_i}, f_{\theta_{i-1}}, \ldots$. The idea of the training is to make the network match the \textit{MCTS} search policy $\boldsymbol{\pi}$ and real outcome of the episode $v$ (not the one estimated at the time of the search!). To do so, we use an MSE loss on the value, a cross-entropy loss on the estimated policy and add $L2$ regularization:
        \begin{equation}
            L(\theta) = \underbrace{(z-v)¬≤}_\text{MSE}+\underbrace{\boldsymbol{\pi}^T\log\mathbf{p}}_\text{cross-entropy}+\underbrace{c_{reg}\lVert\theta\rVert_2}_{L2\text{ reg}}
        \end{equation}
    \end{enumerate} 

    \begin{table}[h]
        \centering
        \begin{tabular}{lll}
            \toprule
            & Parameter & Value \\
            \midrule
            \textbf{Game} & Size of the board & 5 \\
            & Maximum number of walls per player & 5 \\
            & Maximum number of turns & 100 \\
            \midrule
            \textbf{Self-play} & Number of games & $1000$ \\
            & Number of \textit{MCTS} simulations & $200$ \\
            & UCT constant $c_{uct}$ & $1.25$ \\
            & Initial temperature & $1.0$ \\
            & Number of tempered steps & $20$ \\
            \midrule
            \textbf{Training} & Learning rate & $1e$-$3$\\
            & Epochs for each iteration & 100 \\
            &Batch size & 32 \\
            &$L2$ regularization parameter $c_{reg}$ & $1e$-$4$ \\
            \midrule
            \textbf{Model} & Number of filters & $128$\\
            & Number of residual blocks & $15$ \\
            \midrule
            \textbf{Evaluation} & Evaluation time & $2$s \\
            \bottomrule
        \end{tabular}
    \caption{Parameters used in our implementation of AlphaZero}
    \label{tab:alphazero-params}
    \end{table}

    Parameters used for the above-mentioned optimization cycle are shown in table \ref{tab:alphazero-params}. The architecture of the estimation network is given in appendix \ref{sec:alphazero-network} and is directly inspired from the work of \cite{alphazero}. More interestingly, it leverages the 2D geometric structure (and thus invariances) of the \textit{Quoridor} board through a 2D state representation. The previously described state space $s_t$ is indeed turned into a \textit{"plane"}-based representation $F_t$ with the following $3$ feature planes:
    \begin{itemize}
        \item a one-hot encoding plane for the current player (i.e. $0$ everywhere and $1$ at the player plosition)
        \item a one-hot encoding plane for the opponent player 
        \item a plane for walls where $0, 1, 2$ respectively stand for empty, horizontal wall and vertical wall (all things considered, it could also be interesting to try two different feature planes for both horizontal and vertical walls). Note that the plane of walls is smaller than the board dimensions; we therefore pad it with $0$s.
    \end{itemize}  
    As the network needs some temporal knowledge of the state, previous state space feature planes $T-1$ are concatenated and constant feature planes $C_t$ specifying respectively which player is playing (i.e. $0$ for player 0, $1$ for player 1), the number of remaining walls for the current player and the number of remaining walls for the opponent player to provide overall $3\times T + 3$ feature planes: $(F_{t-T+1}, \ldots, F_{t-1}, F_t, C_t)$. In order for the representation to be consistent and for the network to capture the dynamics at play, the concatenated feature planes are then rotated (180¬∞) to match the perspective of the current player. Finally, actions are mapped to indices which means that the features abstracted by the convolutional layers are flattened to obtain the policy vector $\mathbf{p}$ (see appendix \ref{sec:alphazero-network}) for more details.
    
    In evaluation mode, a time parameter can be given and limits the model to human-like evaluation conditions (i.e. tree searches will be performed as long as the agent can do it).


\section{Results and Discussion}
\label{sec:results}

\subsection{MC-RAVE}
For this draft version of the paper, we don't have final results to present. We trained our agent by self-play, as described above, with parameters $k=1000$ and $c=0.1$. Next step will be to evaluate the performance of such a trained agent.

\subsection{AlphaZero}
As of today, we have not yet managed to train a satisfying \textit{AlphaZero}-like agent. The main constraints are the computational resources required to generate self-play games as each of them results in many \textit{MCTS} evaluations for each decision and inherently lots of model inferences. Despite the use of CUDA-supporting hardware, we are still training models and will update our results in the final version of this paper.

Nonetheless, considering the implementation we proposed, we can discuss several of our choices and pave the way for future directions:
\begin{itemize}
    \item Self-play records are not equally interesting in terms of the information they convey about the best strategy to adopt. As a consequence, we could introduce \textit{importance sampling} as proposed in the \textit{MuZero}\cite{muzero} follow-up paper by Deepmind to prioritize decisive moves.
    \item Our action policy is represented by the evaluation network $f_\theta$ as a flattened 1D vector. As such, it is not \textit{equivariant} with respect to the 2D geometric representation of the board and we can hypothesize that the network learns the policy in a sort of "data augmentation" fashion induced by the repetition of self-play records. For more information about \textit{equivariance} and \textit{symmetries} in the context of machine learning, we refer the reader to the theory of \textit{Geometric Deep Learning}\cite{gdl}.
    \item As we quickly observed, the random uniform initialization of our models equally weights actions in the beginning. Consequently, agents tend to exhaust their limited units of walls and thus fail to make any use of them. One heuristic to avoid this could be to deter them from using walls in the first turns of the game.
    \item Convolutions with small kernels do not allow to capture the context of the game in a broad sense, we therefore suggest to use dilated convolution\cite{dilated-conv} to learn multi-scale features, especially when working on a larger board than the reduced one we used.
\end{itemize}

\section{Conclusions}
\label{sec:conclusion}
In this paper, Reinforcement Learning approaches are discussed in the context of the zero-sum two-player game of Quoridor. Various methods were considered but only few suited the constraints imposed by the properties and rules of Quoridor. A Monte-Carlo Tree Search approach is performed through the implementation of MC-RAVE, an AlphaZero-like agent has been implemented and is currently being trained, and a heuristics-based approach is evaluated leveraging Minimax and shortest path searches. Although the obtention of conclusive results is still pending, significant efforts have gone into the agents and the approaches look promising given their successes in the literature. In the remaining time we plan to train all agents and perform a detailed comparative analysis, as well as improve the efficienct of our decision making methods with the implementation of the Alpha-Beta pruning method. Time permitting we would also be very interested in implementing the SARSA method.

\newpage
\section{Appendix}\label{sec:appendix}

\section*{Additional Quoridor Rules}\label{sec:quoridor-details}
A pawn can be moved only horizontally or vertically (\textit{taxi-cab}) and cannot skip over a wall. 
There are two exceptions to the regular taxi-cab moves wherein your pawn is right in front of the opponent's pawn and you'd like to advance in the direction of the opponent's piece. In the case where there is no wall behind the opponent's pawn, you may skip over them. If there is a wall behind them, you can exceptionally move diagonally.

\section*{AlphaZero network configuration}
\label{sec:alphazero-network}
Even though our \textit{AlphaZero}-like network is close to the one initially proposed in \cite{alphagozero}, we chose smaller parameters due to the reduced size of our game space (see table \ref{tab:alphazero-params} for more details). Our implementation uses \textit{PyTorch}\cite{pytorch} and consists in:
\begin{itemize}
    \item a first convolution of $128$ filters of kernel size $3$ with stride $1$ + 2D batch normalization + ReLU activation
    \item a stack of residual blocks made of:
    \begin{itemize}
        \item a convolution of $128$ filters of kernel size $3$ with stride $1$ + 2D batch normalization + ReLU activation
        \item a convolution of $128$ filters of kernel size $3$ with stride $1$ + 2D batch normalization + \textbf{a residual connection} + ReLU activation
    \end{itemize}
    \item two output heads:
    \begin{enumerate}
        \item a \textbf{policy head} made of:
            \begin{itemize}
                \item a convolution of $2$ filters of kernel size $1$ with stride $1$ + 2D batch normalization + ReLU activation
                \item a flattener
                \item a dense layer + softmax output layer
            \end{itemize}
        \item a \textbf{value head} made of:
            \begin{itemize}
                \item a convolution of $1$ filter of kernel size $1$ with stride $1$ + 2D batch normalization + ReLU activation layer
                \item a flattener
                \item a dense layer of size $256$ + ReLU activation
                \item a dense layer to output size $1$ + $\tanh$ activation which restricts the output value in $[-1,1]$
            \end{itemize}
    \end{enumerate}
\end{itemize}
Note that each convolution is padded with 0 in order to give the same output size as the input.

\bibliographystyle{plain}
\bibliography{biblio}

\end{document}
