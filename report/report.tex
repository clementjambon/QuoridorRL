\documentclass[journal, a4paper]{IEEEtran}

\usepackage{graphicx}   
\usepackage{hyperref} 
\usepackage{url}        
\usepackage{amssymb}
\usepackage{amsmath}    
\usepackage{booktabs} % Essential for nice tabs (see https://people.inf.ethz.ch/markusp/teaching/guides/guide-tables.pdf)
\usepackage{xcolor} 

\usepackage{bm}
\newcommand{\vect}[1]{\boldsymbol{\mathbf{#1}}}

% Some useful/example abbreviations for writing math
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\ypred}{\mathbf{\hat y}}
\newcommand{\yp}{{\hat y}}

\newif\ifanonymous
\anonymousfalse

\begin{document}

% Define document title, do NOT write author names for the initial submission
\title{QuoridorRL: solving a two-player strategy game with reinforcement learning}
\ifanonymous
\author{Anonymous Authors}
\else
\author{Nathan Pollet, Rebecca Jaubert, Laura Minkova, Erwan Umlil and Cl√©ment Jambon}
\fi
\maketitle

% Write abstract here
\begin{abstract}
Quoridor is a zero-sum two-player board game which has not yet been studied extensively in the context of reinforcement learning. Its relatively large state and branching complexities and the lack of human-generated data make it very challenging and unsuitable for simple control strategies and exhaustive tree search approaches. Consequently, we provide a full \textit{gym}-like environment for the game and its mechanics. We then introduce three strategies with the aim of reaching human-level control: first a heuristic approach with \textit{MinMax} tree search relying on handcrafted evaluation functions, then a \textit{Monte-Carlo Tree Search} with \textit{Rapid Action Value Estimation} and finally a \textit{Monte-Carlo Tree Search} augmented with an evaluation neural network directly inspired from \textit{AlphaZero}. Due to limited time and computational resources, the latter approach did not yield convincing result. However, we reach human-level strategies with the former two techniques.
\end{abstract}

% Each section begins with a \section{title} command
\section{Introduction}
\label{sec:intro}

In this project, game theory, and in particular reinforcement learning approaches, are considered through the concrete example of Quoridor, a complex two-player zero-sum strategy game designed by Mirko Marchesi and published by Gigamic Games \cite{quoridor-gigamic}. This is a particularly interesting use case as its game complexity is very large (large state and branching complexities), which makes exhaustive tree search approaches impractical for real-time applications. Moreover, finding or generating sufficient amounts of game-related expert data for use in supervised learning methods is a challenging task. Large game complexity is a common challenge in popular games such as chess, Go or Shogi whereas the scarcity of expert game data is rather rare for most of these popular games, the majority of which having been played and studied widely. For instance \textit{AlphaGo} \cite{alphago}, whose successor \textit{AlphaZero} \cite{alphazero} is the basis of one of the three strategies implemented, could not be repurposed for this game scenario due to a lack of proper data. To implement our agents, more suited search approaches are used such as \textit{Monte-Carlo Tree Search} with \textit{Rapid Action Value Estimation} (\textit{MC-RAVE}) \cite{GELLY20111856} and \textit{AlphaZero} \cite{alphazero}. In this paper, prior approaches to provide robust and human-level control agents are reviewed (section \ref{sec:background}), the implementation of the environment is discussed (section \ref{sec:environment}), methods used to solve the environment are detailed (section \ref{sec:models}), the experiments are described, and a discussion of the results is presented (section \ref{sec:results}).
 Although the environment and the agents are successfully implemented, due to the large complexity of the game some agent implementations such as \textit{AlphaZero} require further training and refinements (e.g. minor structural changes, hyperparameter tuning). 
%Lastly, time permitting, additional agent implementations the likes of SARSA, Q-Learning or DQN will be considered in the context of an overall comparative analysis. % should we keep this here? vs in Conclusions?

In addition to this paper, we provide the sources of our code\footnote{Source: \url{https://github.com/clementjambon/QuoridorRL}} along with instructions to train and run each agent in the attached \textit{README} file. 

\section{Background and Related Work}
\label{sec:background}

\subsection{Game and complexity}
\label{ssec:complexity}

Quoridor is a stochastic, perfect information, adversarial game that can be played with two to four players. For the purpose of this project and exploring reinforcement learning methods, however, only the two-player version of the game will be considered. The board game consists of a $n$ by $n$ square-tiled board, where $n$ is odd. 
The two players start off with $f$ walls and one game pawn each. Each player then places their respective pawn in the center of opposing outermost sides of the grid. Both players take turns doing one of two things: moving their pawn by one spot or placing a wall on the board in the spaces between the tiles, making sure the length of the wall is in contact with exactly two tiles on each side. 
The objective of the game is to advance your respective pawn from one end of the board to the other. Players can place the walls anywhere on the grid, but at least one viable path to each side must exist for each player. Details on legal pawn moves can be found in the appendix \ref{sec:quoridor-details}.

As previously mentioned, Quoridor has very large state-space and game tree complexities. The following paragraphs will be dedicated to estimating these values. 

The state-space of a game is defined as all the possible states of the game \cite{heuristic-agent}. Given the difficulty in calculating the number of illegal positions in a given state of Quoridor, an upper bound for all possible states will be calculated. 
This upper bound is equal to the product of the total number of possible pawn positions, $S_{tok}$, and the total number of wall positions, $S_{wall}$.  There are at most $n^{2}$ positions that the first player's pawn can be placed at, and $n^{2} -1$ possible positions on the board for the second player to place their game piece. Therefore $S_{tok} = n^{2}(n^{2}-1) = n^{4} - n^{2}$.

The number of possible wall positions can also be estimated using an upper bound. In a single row, $n-1$ walls can be placed. Since there are $n-1$ rows, there are $(n-1)^{2}$ possible places to put walls on the rows of the grid. One must consider the possible positions of walls in the columns, which as it turns out is the exact same given the square nature of the grid. Therefore there are technically $2(n-1)^{2}$ possible places to put walls in Quoridor. 
Placing a wall at any position on the grid, however, takes away at most four possible wall positions from the total possible spots previously calculated. Thus, for the $2f$ fences that can be placed on the board (since each player has $f$ rectangular tiles, there are at most $\sum_{i=0}^{2f} \prod_{j=0}^{i}(2(n-1)^2 - 4i)$ possible placements for the walls. This is clearly a loose upper bound as it misses many other illegal positions of walls, but it does give an idea of the scale of the number of possible walls. Finally, the state space is equal to $2(n-1)^{2} * \sum_{i=0}^{2f} \prod_{j=0}^{i}(2(n-1)^2 - 4i)$, which can grow incredibly large as the size of the board increases.

The size of a game tree is the number of possible games that can be played. This can be estimated by the value $b^{d}$ where $b$ is the average number of branches there are from a node in the game tree (i.e possible positions from a given state), and $d$ is the average number of turns that a game takes (i.e the average depth of the game tree). One can see that as the board gets larger, the number of possible moves from a state space grows due to the increasing number of possible positions for both pawn and walls, as does the number of turns a game may take since the token will have to make increasingly more steps to get to the other side. 
Therefore, the size of the game tree grows exponentially. For a 9 by 9 board, for example, the size of the game-tree can be estimated as $60.4^{91.1} \approx 1.7884 * 10^{162}$ \cite{mastering-quoridor}, which makes exhaustive search impossible.

% https://math.stackexchange.com/questions/953750/counting-all-possible-board-positions-in-quoridor

\subsection{MCTS}
\label{ssec:mcts}

\textit{Monte-Carlo Tree Search} (\textit{MCTS})\cite{mcts-review} is a search method that combines the precision of tree search with the generality of random sampling. Due to its property of growing the tree asymmetrically by concentrating on the more promising subtrees, it is particularly well suited for games with a high branching factor. Moreover, it can function with little or no domain knowledge and is a particularly proficient approach for games and problems which can be represented as trees of sequential decisions. \textit{MCTS} operates by taking random samples in the decision space and building a search tree according to the results. The basic \textit{MCTS} process is straightforward. The tree is built incrementally and asymmetrically. For each iteration of the algorithm, \textit{MCTS} leverages a tree policy in order to select the "best" node for the current tree for expansion. A good policy has a proper balance between exploration (explore yet unsampled areas) and exploitation (explore promising areas). Then, a simulation is ran from the selected node and the (current) search tree is updated from its results (addition of a child node, update of the statistics of its ancestors). 

Various modifications of the basic \textit{Monte-Carlo Tree Search} method have been proposed, mostly to shorten the search time. Certain improvements attempt to, when applicable, use a heavy payout to incorporate various heuristics that influence the choices of selected nodes (i.e. the choices of moves). These heuristics could employ domain knowledge or use the results of previous playouts (e.g. the \textit{Last Good Reply heuristic}, \textit{history heuristics}). Another approach is to employ domain knowledge when building the game tree to help the exploitation of some variants. Such an approach is related to the concept of \textit{progressive bias}. Finally, using the \textit{Rapid Action Value Estimation} (\textit{RAVE}) can reduce the exploratory phase of the algorithm significantly by modifying the statistics which is being stored at the nodes of interest (i.e. modified \textit{UCB1} formula). This approach is being implemented for one of the agents. 

\subsection{Training without human knowledge}
\label{ssec:human-knowledge}
Unlike many other games of similar complexity (e.g. Chess, Go, etc), QuoridorRL has not been extensively studied and thus lacks precisely handcrafted features and evaluation functions which makes the use of fixed-depth an heuristic-based MinMax searches unreliable. As a consequence and unlike AlphaGo\cite{alphago} where input features are carefully designed by humans, we turned to a more generic approach similar to AlphaZero\cite{alphazero} where inputs are simple feature planes of the board (see \ref{ssec:alphazero} for more details) and the neural network learns itself to capture features through residual convolutions of the game board. 

In order to perform \textit{Monte-Carlo Tree Search} over the large complexity space of Quoridor and considering the lack of game data contrary to well-known and already analysed games, we faced the challenge of training our model without human knowledge.  To this extent, we chose a \textit{"self-play"} approach similar to the one initially presented in  \textit{AlphaGoZero}\cite{alphagozero} and later expanded to a larger set of games (namely Chess and Shogi) in \textit{AlphaZero}\cite{alphazero}.

\section{Environment}
\label{sec:environment}

Given that Quoridor is a lesser-known board game, to our knowledge, there are no public implementations of its mechanics for the purpose of training RL agents. Consequently, we provide with this project a turn-based \textit{gym}-like (cf. \cite{openai-gym}) environment.

In order to limit the computational burden due to the very large complexity of the game in its original configuration, we chose to simplify the state space by restricting ourselves to the following rules:
\begin{itemize}
    \item the board has size $5\times 5$ (instead of $9\times 9$)
    \item each player can add at most $5$ walls (instead of $10$)
    \item the game can last at most $50$ turns, after which it is considered a "draw"
\end{itemize}

\subsection{Logic and representation}
\label{ssec:game-logic}

Although the game logic could be implemented by relying on a graph as suggested in \cite{heuristic-agent}, we chose a different intrinsic representation to limit the memory and computational footprint which are critical for efficient \textit{Monte-Carlo Tree Searches}. We coded the game logic from scratch and provided an interface that allows users to check whether an action is valid or not, fetch all valid actions, and take actions based on the abstract or string representation mentioned below. In order to check valid actions, we use the well-known $A^*$ algorithm (see chapter 3 of \cite{russel2010} for more details) in order to perform pathfinding. Note that, initially, every time we tested an action, a pathfinding was performed even when little changes on the board had occured from one state to the other. To address this, we added a custom union-find structure to track connected components of walls on the board. As a result, we now only search paths when a connected-component linking two (potentially equal) sides of the board are merged.

Intrinsiquely, the state $s\in\mathcal{S}$ of the game is represented as the set of player positions on the board, the number of remaining walls for each player, the index of the current player and a 2D array storing walls located by their lower left-hand positions and where $-1, 0, 1$ stands respectively for empty, horizontal ($x$-aligned), vertical ($y$-aligned).

States and actions can also be represented with a \textit{string} notation. Ours is inspired from the algebraic notation presented in \cite{quoridor-wikipedia}. More precisely, the $x$-coordinate of states are indexed in $\{1, \ldots, grid\_ size\}$ while the $y$-coordinate of states are indexed in $\{a, b, c, \ldots\}$. Walls are described in a similar fashion by appending the direction ($h$ for horizontal and $v$ for vertical) to the number/letter position couple. As such, we can represent player actions by the target \textit{string} position when the player performs a pawn move or the target wall position with the direction suffix when the player adds a wall. For completeness, we add the number of used walls to the state, the current player and number of played steps at the end of the \textit{string} representation. Equation \ref{eq:state-str} provides an example of such a representation and is illustrated in Figure \ref{fig:state-str}.
\begin{equation}
    \label{eq:state-str}
    % 1d;5c;1dv;2bh;3av;3bv;4dv;p0:2;p1:3;p0;
    \underbrace{\text{1d;5c;}}_\text{pawn positions}\overbrace{\text{1dv;2bh;3av;3bv;4dv;}}^\text{walls}\underbrace{\text{p0:2;p1:3;}}_\text{used walls}\overbrace{\text{p0;}}^\text{current player}\underbrace{\text{t:28;}}_\text{time steps}
\end{equation}
\begin{figure}
    \centering
    \includegraphics[width=0.25\textwidth]{figures/state-str.png}
    \caption{Board representation of the state given in equation \ref{eq:state-str}}
    \label{fig:state-str}
\end{figure}

\subsection{User interface}
In order to visualize and play games abstracted by the aforementioned logic, we provide a user interface with parameters that can be tuned according to the chosen configuration of the game. To this extent, we used the python \textit{SDL}-based library \textit{PyGame}\cite{pygame}. Our relatively simple, yet functional \textit{GUI} is shown in Figure \ref{fig:gui} in the appendix.

\section{Models}
\label{sec:models}

\subsection{Decision Making, MinMax and Heuristics}\label{ssec:heuristics}

This section presents a basic \textit{decision making} approach we propose to solve the game of Quoridor. There are two strategies our agent can take whose usages depend on two factors: the first being that you have no more walls to place on the board, and the second being you still do.
For the former, if you cannot place anymore walls on the board the agent proceeds by moving in the shortest-path manner which is calculated with the \textit{dijkstra} algorithm. 
For the latter, we decide what our next move is using a \textit{MinMax} tree.

\textit{MinMax} is a classical method for \textit{decision making} in zero-sum game settings. Every edge in the tree is an action in the game and every node stores the evaluation of a particular state of the game.
Ideally, a full \textit{MinMax} tree would be computed from which, starting from the leaf nodes, you choose the action that maximizes or minimizes the reward depending on which player you are. As previously mentionned in \ref{ssec:complexity}, Quoridor has a very large game tree. For this reason, directly inspired from \cite{heuristic-agent}, we chose to compute the \textit{MinMax} tree only up to a certain depth, and evaluate a game state given four different heuristics: 1. $f_1$ the position of the current player, 2. $f_2$ the position difference of the players, 3. $f_3$ the number of required moves for the current player to get to the next column and 4. $f_4$ the number of required moves for the opposing player to get to the next column. These four features are then combined into a single game state value using the following formula:  
$$Eval(s) = w_1 f_1 + w_2 f_2 + w_3 f_3 - w_4 f_4,   w_i \in \mathbb{R}$$
This results in two things: a \textit{MinMax} search algorithm with moderate runtime, and game state evaluation heuristics that can be used not only \textit{MinMax} but for \textit{MCTS-RAVE} as well.

\subsection{MC-RAVE}
\label{ssec:mc-rave}
In order to solve complex two-player strategy games, \textit{MCTS} has proven to be useful (cf. \cite{mcts-review}). However, when the state and branching complexities grow large, more samples need to be generated. \textit{MC-RAVE}\cite{mc-rave} tackles this issue by extrapolating on data collected with respect to the action space.

The main issue with this MCTS, described in \ref{ssec:mcts}, lies in the huge number of simulations required to estimate the action value $Q(s,a)$. \textit{Rapid Action Value Estimation} (\textit{RAVE}) \cite{mc-rave} tackles this by providing a way to quickly estimate the value of an action thanks to the \textit{all-moves-as-first} (\textit{AMAF}) heuristic. The main idea is to assume that each action has a global value, no matter when it is played. Therefore, we can estimate the value of an action $a$ in state $s$ as follows:
$$\tilde Q(s,a) = \frac{1}{\tilde N(s,a)} \sum_{i=1}^{N(s)} \mathbf{\tilde 1}_{i}(s,a)z_i$$
where $\mathbf{\tilde 1}_{i}(s,a)$ is an indicator function returning 1 if state $s$ was encountered at any step $t$ of the $i$-th simulation, and action $a$ was selected at any state $u\geq t$, and $\tilde N(s,a):=\sum_{i=1}^{N(s)} \mathbf{\tilde 1}_{i}(s,a)$, the number of simulations used to estimate the \textit{AMAF} value. In \textit{RAVE}, rather than looking at simulations where action $a$ was selected in state $s$, we look at all simulations where action $a$ was selected \textit{after} state $s$. It is faster to compute, but less accurate.

\textit{MC-RAVE} simply consists in a trade-off between conventional \textit{MCTS} and \textit{RAVE}, by mixing both estimated action values:
$$Q_*(s,a) = (1-\beta(s,a))Q(s,a) + \beta(s,a)\tilde Q(s,a)$$
We use a handcrafted schedule proposed in the same paper\cite{mc-rave}:
$$\beta(s,a) = \sqrt[]{\frac{k}{3N(s)+k}}$$
with $k=1000$.

Finally, we use \textit{UCT-RAVE} \cite{mc-rave} to favor exploration:
$$Q_*^+(s,a) = Q_*(s,a) + c\:\:\sqrt[]{\frac{\log N(s)}{N(s,a)}}$$
where $c$  is a mixing parameter.

\subsection{AlphaZero agent}
\label{ssec:alphazero}
    To alleviate the computational burden of tree search, \textit{AlphaZero}\cite{alphazero} proposes to perform \textit{MCTS} where, instead of full rollouts like in \textit{MC-RAVE}, nodes are progressively expanded thanks to a single policy-value evaluation network $f_\theta(s)=(\mathbf{p}, v)$ that takes as input a state $s\in \mathcal{S}$ and provides an action policy $\mathbf{p}\in\mathcal{P}(\mathcal{A})$ (see appendix \ref{sec:alphazero-network} for the detailed architecture). Similarly to conventional \textit{MCTS}, each node represents a state and a edge stands for a state-action pair whose attributes are its number of visit, $N(s,a)$, the accumulated action value , $W(s,a)$, the average action value, $Q(s,a)$, and the prior probability of this pair, $P(s,a)$. With this setup, we follow iteratively three consecutive steps:
    \begin{enumerate}
        \item \textbf{Search:} we start from an uninitialized tree whose single root node is the current state $s_0$ of the simulation. For every simulation, as long as a leaf has not been reached, the tree is explored by choosing actions according to the \textit{UCT} (Upper Confidence bounds applied to Trees) algorithm that treats the tree as a multi-armed bandit problem and maximizes an upper confidence bound on the value of actions : $$a_t=\argmax_{a\in\mathcal{A}}(Q(s_t, a)+c_{uct}P(s_t,a)\frac{\sqrt{\sum_{b\in\mathcal{A}}N(s_t, b)}}{1+N(s_t,a)})$$ where $c_{uct}$ tempers mixing between greedy exploitation and exploration of the tree.
        \item \textbf{Expansion:} everytime a leaf $s_L$ is encountered in the rollout, it is expanded by adding edges following the estimation provided by the neural network $f_\theta(s_L)=(\mathbf{p_L}, v_l)$ i.e each edge is initialized with $N(s_L, a)=W(s_L,a)=Q(s_L,a)=0$ and $P(s_L,.)=\mathbf{p_L}$.
        \item \textbf{Backup:} the action values and visits of each rollout are then back-propagated following $N(s_t, a_t) = N(s_t, a_t) + 1$, $W(s_t, a_t) = W(s_t, a_t) + v$ and $Q(s_t, a_t)=\frac{W(s_t, a_t)}{N(s_t, a_t)}$
    \end{enumerate}


    Unlike \textit{AlphaGo}\cite{alphago} whose expansion was guided by a network trained by supervision on real human-generated data, \textit{AlphaZero} uses self-play schemes to generate training samples without human knowledge, making it particularly interesting in the context of Quoridor for which we have no collected data. We adapted these schemes to our Quoridor environment as follows. We first randomly initialize the evaluation network $f_{\theta_0}$ with random weights $\theta_0$ and then proceed iteratively:
    \begin{enumerate}
        \item the last trained network $f_{\theta_i}$ is used to generate self-play games where each move is chosen according to a \textit{MCTS} as described above. More precisely, actions are chosen following the search policy given by the visit counts of the root edges accumulated during the search and tempered with a temperature parameter $\tau$ i.e. $\boldsymbol{\pi}(.|s_0)=\frac{N(s_0,.)^\frac{1}{\tau}}{\sum_{b\in\mathcal{A}}N(s_0, b)^\frac{1}{\tau}}$ and we record state-action-outcome tuples $(s_t, \boldsymbol{\pi}_t, z)$ where $z$ is $1, -1, 0$ respectively for a victory, a defeat or a draw.
        \item the network is $f_{\theta_{i+1}}$ is initialized with the weights of $f_{\theta_i}$ and trained with batches of self-play records sampled from previous models i.e $f_{\theta_i}, f_{\theta_{i-1}}, \ldots$. The idea of the training is to make the network match the \textit{MCTS} search policy $\boldsymbol{\pi}$ and real outcome of the episode $v$ (not the one estimated at the time of the search!). To do so, we use an MSE loss on the value, a cross-entropy loss on the estimated policy and add $L2$ regularization:
        \begin{equation}
            L(\theta) = \underbrace{(z-v)¬≤}_\text{MSE}+\underbrace{\boldsymbol{\pi}^T\log\mathbf{p}}_\text{cross-entropy}+\underbrace{c_{reg}\lVert\theta\rVert_2}_{L2\text{ reg}}
        \end{equation}
    \end{enumerate} 

    \begin{table}[h]
        \centering
        \begin{tabular}{lll}
            \toprule
            & Parameter & Value \\
            \midrule
            \textbf{Game} & Size of the board & 5 \\
            & Maximum number of walls per player & 5 \\
            & Maximum number of time steps & 50 \\
            \midrule
            \textbf{Self-play} & Number of games & $100$ \\
            & Number of \textit{MCTS} simulations & $100$ \\
            & UCT constant $c_{uct}$ & $1.25$ \\
            & Initial temperature & $1.0$ \\
            & Number of tempered steps & $20$ \\
            \midrule
            \textbf{Training} & Learning rate & $1e$-$3$\\
            & Epochs for each iteration & 100 \\
            &Batch size & 32 \\
            &$L2$ regularization parameter $c_{reg}$ & $1e$-$4$ \\
            \midrule
            \textbf{Model} & Number of filters & $64$\\
            & Number of residual blocks & $9$ \\
            \midrule
            \textbf{Evaluation} & Evaluation time & $2$s \\
            \bottomrule
        \end{tabular}
    \caption{Parameters used in our implementation of AlphaZero}
    \label{tab:alphazero-params}
    \end{table}

    Parameters used for the above-mentioned optimization cycle are shown in table \ref{tab:alphazero-params}. The architecture of the estimation network is given in appendix \ref{sec:alphazero-network} and is directly inspired from the work of \cite{alphazero}. More interestingly, it leverages the 2D geometric structure (and thus invariances) of the Quoridor board through a 2D state representation. The previously described state space $s_t$ is indeed turned into a \textit{``plane"}-based representation $F_t$ with the following $3$ feature planes:
    \begin{itemize}
        \item a one-hot encoding plane for the current player (i.e. $0$ everywhere and $1$ at the player plosition)
        \item a one-hot encoding plane for the opponent player 
        \item a plane for walls where $0, 1, 2$ respectively stand for empty, horizontal wall and vertical wall (all things considered, it could also be interesting to try two different feature planes for both horizontal and vertical walls). Note that the plane of walls is smaller than the board dimensions; we therefore pad it with $0$s.
    \end{itemize}  
    As the network needs some temporal knowledge of the state, $T-1$ previous state space feature planes are concatenated and constant feature planes $C_t$ specifying respectively which player is playing, the number of remaining walls for the current player and the number of remaining walls for the opponent player to provide overall $3\times T + 3$ feature planes: $(F_{t-T+1}, \ldots, F_{t-1}, F_t, C_t)$. In order for the representation to be consistent and for the network to capture the dynamics at play, the concatenated feature planes are then rotated (180¬∞) to match the perspective of the current player. Finally, actions are mapped to indices thus flattening convolutional layers features to obtain the policy vector $\mathbf{p}$ (see appendix \ref{sec:alphazero-network} for more details). In evaluation mode, a time parameter can be given and limits the model to human-like evaluation conditions (i.e. tree searches will be performed as long as the agent can do it).


\section{Results and Discussion}
\label{sec:results}

\subsection{MC-RAVE}
We played against our agent with parameters $k=1000$, $c=100.0$ and $1000$ iterations per turn. These values empirically allowed to have a performant bot. We provide two modes: one is self-play, which consists in playing bot versus bot ; the second one, more interesting here, is human vs bot. Due to the large number of possible actions, we need to perform 1000 iterations per turn. As a consequence, runtimes are significantly affected (see Figure \ref{sec:appendix}). Furthermore, note that the decision time depends on how close we are to the end of the game (see Figure \ref{fig:game}).

\begin{table}[h]
        \centering
        \begin{tabular}{llll}
            \toprule
            \textbf{Number of iterations} & 10 & 100 & 1000 \\
            \midrule
            \textbf{Average time to decide (s)} & 1.5 & 9.16 & 176.9 \\
            \bottomrule
        \end{tabular}
    \caption{MC-RAVE: Average time to decide a move for different numbers of iterations}
    \label{tab:execution_time}
    \end{table}

However, as a player we feel like if we were playing against a human. Indeed, as an experiment, we played human vs bot, allowing the human only to move its pawn. The purposes of this experiment are to show that our bot is able to use walls to be better than a shortest path algorithm, and to win even if he does not play first. Such a game is presented in Figure \ref{fig:game}. As we can see, the bot (in black) seems to develop a complex strategy to win.

These results are very satisfying. The main issue remains speed, and this could be partly solved by using a machine with higher performances.

\subsection{AlphaZero}
As of today, we have not yet managed to train a satisfying \textit{AlphaZero}-like agent. The main constraints are the computational resources required to generate self-play games as each of them results in many \textit{MCTS} evaluations for each decision and inherently lots of model inferences. Despite the use of CUDA-supporting hardware, we did not manage to train our model with satisfying results.

In addition to the optimization of the environment runtime crucial for fast \textit{MCTS} and detailed in paragraph \ref{ssec:game-logic}, we tried to speed up training by introducing a new intermediate reward based on the relative shortest path of both players. More precisely, at each time step, we optionaly provided the agent with a reward defined as: 
\begin{equation}
    r_t =\frac{\text{shortest path}_\text{opponent}-\text{shortest path}_\text{current player}}{n^2}
\end{equation}
where $n$ is the size of the board. In the time allocated for this project, this extension did not yield satisfying results either. However, we provide all required resources and code to train and test both models (see \textit{README}\footnote{\url{https://github.com/clementjambon/QuoridorRL}}).

Furthermore, considering the implementation we proposed, we can discuss several of our choices and pave the way for future directions:
\begin{itemize}
    \item Self-play records are not equally interesting in terms of the information they convey about the best strategy to adopt. As a consequence, we could introduce \textit{importance sampling} as proposed in the \textit{MuZero}\cite{muzero} follow-up paper by DeepMind to prioritize decisive moves.
    \item Our action policy is represented by the evaluation network $f_\theta$ as a flattened 1D vector. As such, it is not \textit{equivariant} with respect to the 2D geometric representation of the board and we can hypothesize that the network learns the policy in a sort of "data augmentation" fashion induced by the repetition of self-play records. For more information about \textit{equivariance} and \textit{symmetries} in the context of machine learning, we refer the reader to the theory of \textit{Geometric Deep Learning}\cite{gdl}.
    \item As we quickly observed, the random uniform initialization of our models equally weights actions in the beginning. Consequently, agents tend to exhaust their limited units of walls and thus fail to make any use of them. One heuristic to avoid this could be to deter them from using walls in the first turns of the game.
    \item Convolutions with small kernels do not allow to capture the context of the game in a broad sense, we therefore suggest the use dilated convolution\cite{dilated-conv} to learn multi-scale features, especially when dealing with a larger board than the reduced one we used.
\end{itemize}

\section{Conclusions}
\label{sec:conclusion}
In this paper, Reinforcement Learning approaches are discussed in the context of the zero-sum two-player game of Quoridor. Various methods were considered but only few suited the constraints imposed by the properties and rules of Quoridor. A \textit{Monte-Carlo Tree Search} approach is performed through the implementation of \textit{MC-RAVE} providing human-level responses, an \textit{AlphaZero}-like agent has been fully implemented, and a heuristics-based approach is evaluated leveraging \textit{MinMax} and \textit{pathfinding}. Although the obtention of conclusive results is still pending for the \textit{AlphaZero} approach despite multiple attempts to improve both learning and runtime, significant efforts have gone into its design. 

\newpage
\section{Appendix}\label{sec:appendix}

\section*{Additional Quoridor Rules}\label{sec:quoridor-details}
A pawn can be moved only horizontally or vertically (\textit{taxi-cab}) and cannot skip over a wall. 
There are two exceptions to the regular taxi-cab moves wherein your pawn is right in front of the opponent's pawn and you'd like to advance in the direction of the opponent's piece. In the case where there is no wall behind the opponent's pawn, you may skip over them. If there is a wall behind them, you can exceptionally move diagonally.

\section*{AlphaZero network configuration}
\label{sec:alphazero-network}
Even though our \textit{AlphaZero}-like network is close to the one initially proposed in \cite{alphagozero}, we chose smaller parameters due to the reduced size of our game space (see table \ref{tab:alphazero-params} for more details). Our implementation uses \textit{PyTorch}\cite{pytorch} and consists in:
\begin{itemize}
    \item a first convolution of $64$ filters of kernel size $3$ with stride $1$ + 2D batch normalization + ReLU activation
    \item a stack of residual blocks made of:
    \begin{itemize}
        \item a convolution of $64$ filters of kernel size $3$ with stride $1$ + 2D batch normalization + ReLU activation
        \item a convolution of $64$ filters of kernel size $3$ with stride $1$ + 2D batch normalization + \textbf{a residual connection} + ReLU activation
    \end{itemize}
    \item two output heads:
    \begin{enumerate}
        \item a \textbf{policy head} made of:
            \begin{itemize}
                \item a convolution of $2$ filters of kernel size $1$ with stride $1$ + 2D batch normalization + ReLU activation
                \item a flattener
                \item a dense layer + softmax output layer
            \end{itemize}
        \item a \textbf{value head} made of:
            \begin{itemize}
                \item a convolution of $1$ filter of kernel size $1$ with stride $1$ + 2D batch normalization + ReLU activation layer
                \item a flattener
                \item a dense layer of size $64$ + ReLU activation
                \item a dense layer to output size $1$ + $\tanh$ activation which restricts the output value in $[-1,1]$
            \end{itemize}
    \end{enumerate}
\end{itemize}
Note that each convolution is padded with 0 in order to give the same output size as the input.

\begin{figure}
    \centering
    \includegraphics[width=0.3\textwidth]{figures/gui.png}
    \caption{Provided user interface}
    \label{fig:gui}
\end{figure}


\section*{Game against MC-RAVE}
\label{sec:game}
We (white) played against our MC-RAVE agent (black). We decided to use only moving actions, while MC-RAVE was free to play according to the rules of Quoridor. Despite playing what could be considered as one of the best move at each step as a human, MC-RAVE agent won this game. Figure \ref{fig:game} shows each step of the game, with runtime when the agent plays.

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{figures/game.png}
    \caption{Game Human vs MC-RAVE with its decision time in minutes}
    \label{fig:game}
\end{figure}

\bibliographystyle{plain}
\bibliography{biblio}

\end{document}
